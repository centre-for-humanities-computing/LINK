{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Welcome to our Hands-On Workshop! We'll mostly be exploring classification (predictive modeling) as a\n",
    "# way to study and analyze corpora of literary texts, poetry in particular.\n",
    "\n",
    "##### INSTALLATION!\n",
    "\n",
    "#### If you haven't installed Python yet, I'd like you to download and install the Anaconda distribution, which\n",
    "# can be found here: https://docs.anaconda.com/anaconda/install/\n",
    "# There are options for both MAC and Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA!\n",
    "\n",
    "# I circulated a link to the data in preparation for this workshop. Please download the data and put it somewhere\n",
    "# you can find on your machine. The data is out of copyright by American standards, so you are free to share it or\n",
    "# use it.\n",
    "\n",
    "# A word or two about MASSES versus OTHERS, two important American literary journals from the Modernist period\n",
    "# i.e, 1910 to 1925 or so. Masses was an important left-wing, proletarian journal; Others was a flagship journal\n",
    "# of high modernism, especially Imagism. TS Eliot, Pound, Amy Lowell all published there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### OK let's get going\n",
    "\n",
    "# First, let's just get to know our data!\n",
    "\n",
    "# Let's read in all the metadata and data\n",
    "# Some necessary imports\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# supress warnings, do not do this at home, but it makes the notebook look better (KLN)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "meta1 = 'MassesData.csv'\n",
    "meta2 = 'OthersData.csv'\n",
    "\n",
    "mass_meta = pd.read_csv(meta1, encoding='latin1')\n",
    "# Let's get rid of any rows that don't have FILENAME\n",
    "mass_meta = mass_meta.dropna(axis=0, subset=['FILENAME'])\n",
    "\n",
    "other_meta = pd.read_csv(meta2, encoding='latin1')\n",
    "other_meta = other_meta.dropna(axis=0, subset=['FILENAME'])\n",
    "\n",
    "# Sanity check\n",
    "#mass_meta.head()\n",
    "#other_meta.head()\n",
    "\n",
    "mass_meta.shape, other_meta.shape, mass_meta.columns, other_meta.columns\n",
    "#mass_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a really nice data structure but we're not done. Let's read in the actual\n",
    "# text files into the dataframe, and then we'll have ALL of the data (meta + text) in one place!\n",
    "\n",
    "mass_path = 'Masses/'\n",
    "other_path = 'Others/'\n",
    "\n",
    "import codecs\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "mass_meta['TEXT'] = ''\n",
    "other_meta['TEXT'] = ''\n",
    "\n",
    "for index, row in mass_meta.iterrows():\n",
    "    filepath = mass_path + str(row['FILENAME'])\n",
    "    text = codecs.open(filepath, \"r\")         \n",
    "    raw = text.read()\n",
    "    raw = raw.lower()\n",
    "    raw = ''.join(ch for ch in raw if ch not in exclude)\n",
    "    raw1 = raw.split()\n",
    "    mass_meta.set_value(index, 'TEXT', raw1)\n",
    "    \n",
    "for index, row in other_meta.iterrows():\n",
    "    filepath = other_path + str(row['FILENAME'])\n",
    "    text = codecs.open(filepath, \"r\")         \n",
    "    raw = text.read()\n",
    "    raw = raw.lower()\n",
    "    raw = ''.join(ch for ch in raw if ch not in exclude)\n",
    "    raw1 = raw.split()\n",
    "    other_meta.set_value(index, 'TEXT', raw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "mass_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's learn a bit about our two corpora; let's get a baseline of comparison to understand\n",
    "# how they might be similar or different\n",
    "\n",
    "# First, average year\n",
    "mass_meta['YEAR'].mean(), other_meta['YEAR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's next get average length of title + length of poem\n",
    "\n",
    "mass_meta['TITLE_LENGTH'] = ''\n",
    "mass_meta['LENGTH'] = ''\n",
    "other_meta['TITLE_LENGTH'] = ''\n",
    "other_meta['LENGTH'] = ''\n",
    "\n",
    "for index, row in mass_meta.iterrows():\n",
    "    title = len(row['POEM TITLE'].split())\n",
    "    mass_meta.set_value(index, 'TITLE_LENGTH', title)\n",
    "    text = len(row['TEXT'])\n",
    "    mass_meta.set_value(index, 'LENGTH', text)\n",
    "    \n",
    "for index, row in other_meta.iterrows():\n",
    "    title = len(row['POEM TITLE'].split())\n",
    "    other_meta.set_value(index, 'TITLE_LENGTH', title)\n",
    "    text = len(row['TEXT'])\n",
    "    other_meta.set_value(index, 'LENGTH', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "mass_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, compute average title length, average text length\n",
    "#mass_meta['TITLE_LENGTH'].mean(), other_meta['TITLE_LENGTH'].mean()\n",
    "mass_meta['LENGTH'].mean(), other_meta['LENGTH'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_meta['TITLE_LENGTH'].mean(), other_meta['TITLE_LENGTH'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some more information, working at the full corpus level\n",
    "\n",
    "all_mass = []\n",
    "\n",
    "for index, row in mass_meta.iterrows():\n",
    "    all_mass.append(row['TEXT'])\n",
    "    \n",
    "all_mass = [item for sublist in all_mass for item in sublist]\n",
    "\n",
    "all_other = []\n",
    "\n",
    "for index, row in other_meta.iterrows():\n",
    "    all_other.append(row['TEXT'])\n",
    "    \n",
    "all_other = [item for sublist in all_other for item in sublist]\n",
    "\n",
    "# Sanity check\n",
    "len(all_mass), all_mass[0:5], len(all_other), all_other[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get Type-Token Ratio (TTR) for both corpora\n",
    "\n",
    "TTR_mass = len(set(all_mass)) / len(all_mass)\n",
    "TTR_other = len(set(all_other)) / len(all_other)\n",
    "\n",
    "TTR_mass, TTR_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's identify the most common words in each corpus\n",
    "\n",
    "from collections import Counter\n",
    "mass_counts = Counter(all_mass)\n",
    "other_counts = Counter(all_other)\n",
    "\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "\n",
    "# And then some simple counting\n",
    "ranked_mass = sorted(mass_counts.items(), key=itemgetter(1), reverse=True)\n",
    "ranked_other = sorted(other_counts.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "ranked_mass[0:10], ranked_other[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing stopwords\n",
    "\n",
    "text_file = open(\"jockers_stopwords.txt\", \"r\")\n",
    "jockers_words = text_file.read().split()\n",
    "\n",
    "all_mass2 = [word for word in all_mass if word not in jockers_words]\n",
    "all_other2 = [word for word in all_other if word not in jockers_words]\n",
    "\n",
    "mass_counts2 = Counter(all_mass2)\n",
    "other_counts2 = Counter(all_other2)\n",
    "\n",
    "ranked_mass2 = sorted(mass_counts2.items(), key=itemgetter(1), reverse=True)\n",
    "ranked_other2 = sorted(other_counts2.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "ranked_mass2[0:10], ranked_other2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok one more thing; let's determine which words are in MASS but not OTHER and vice versa!\n",
    "\n",
    "mass_not_other = []\n",
    "\n",
    "for word in all_mass2:\n",
    "    if word not in all_other2:\n",
    "        mass_not_other.append(word)\n",
    "        \n",
    "other_not_mass = []\n",
    "\n",
    "for word in all_other2:\n",
    "    if word not in all_mass2:\n",
    "        other_not_mass.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mass_not_other), len(other_not_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_counts3 = Counter(mass_not_other)\n",
    "other_counts3 = Counter(other_not_mass)\n",
    "\n",
    "ranked_mass3 = sorted(mass_counts3.items(), key=itemgetter(1), reverse=True)\n",
    "ranked_other3 = sorted(other_counts3.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "ranked_mass3[0:10], ranked_other3[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SUMMARY\n",
    "# OK that's enough of that surface analysis!\n",
    "# So we know a few things now about these two corpora and how they are similar/different\n",
    "# We know about their average years; their average title and poem lengths\n",
    "# About their basic lexical diversity/repetitiveness, and total number of words\n",
    "# Which words are most common and which words are in one corpus and not in another\n",
    "\n",
    "## This is important to start developing a basic intuition as to these two corpora before we do\n",
    "# more complex analysis, i.e. classification. Before we throw a complex model at the data and we\n",
    "# develop layers of mediation, I think it's important to first know what's there more directly,\n",
    "# before we run all the data through a machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's start classification\n",
    "\n",
    "# First let's merge our two metadata dataframes into one\n",
    "\n",
    "all_meta = pd.concat([mass_meta, other_meta])\n",
    "# Sanity check\n",
    "mass_meta.shape, other_meta.shape, all_meta.shape\n",
    "all_meta = all_meta.reset_index()\n",
    "all_meta = all_meta.drop('index', 1)\n",
    "all_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK next we have to build the document text matrix (DTM) for classification\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus_path = 'ALL_TEXTS'\n",
    "\n",
    "# Build DTM\n",
    "vectorizer = CountVectorizer(input='filename', min_df=3, encoding='utf8')\n",
    "dtm = vectorizer.fit_transform(corpus_path + \"/\" + all_meta['FILENAME'])\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "matrix.shape, len(vocab), all_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things are about to get complicated so let's first reduce our metadata to just what we need\n",
    "all_meta = all_meta[['AUTHOR', 'JOURNAL', 'YEAR', 'POEM TITLE', 'FILENAME']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's merge the DTM with the metadata, so we have everything we need for classification\n",
    "\n",
    "DTM = pd.DataFrame(matrix, columns=vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat([all_meta, DTM], axis=1)\n",
    "final_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK now we have everything we need for classification/predictive modeling\n",
    "# I'm going to implement the ONE VS ALL method (mini lecture on that) to produce stable results\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "output = []\n",
    "\n",
    "i=0\n",
    "for i in range(final_df.shape[0]):\n",
    "    # First grab all data minus test case, and then, the test case\n",
    "    predict_row = final_df.loc[[i]]\n",
    "    train_rows = final_df.drop(i)\n",
    "    \n",
    "    # Specify logit model, l1 penalty and C=1.0 (standard)\n",
    "    model = LogisticRegression(penalty='l1', C=1)\n",
    "    \n",
    "    # Fit the model\n",
    "    X = train_rows.iloc[:, 5:]\n",
    "    y = train_rows.iloc[:, 1]\n",
    "    TEST_CASE = predict_row.iloc[:, 5:]\n",
    "    true_label = predict_row.iloc[:, 1].values\n",
    "    true_fname = predict_row.iloc[:, 4].values\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict\n",
    "    # predict class labels for the test set\n",
    "    predicted = model.predict(TEST_CASE)\n",
    "    # generate class probabilities\n",
    "    probs = model.predict_proba(TEST_CASE)\n",
    "    \n",
    "    # Save output\n",
    "    output.append((str(true_fname), str(true_label), str(predicted), probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's parse this output so we can analyze it\n",
    "\n",
    "files = []\n",
    "trues = []\n",
    "predicts = []\n",
    "probs = []\n",
    "\n",
    "for item in output:\n",
    "    files.append(item[0])\n",
    "    trues.append(item[1])\n",
    "    predicts.append(item[2])\n",
    "    probs.append(item[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe for easy viewing\n",
    "df = pd.DataFrame(files, columns=['FILENAME'])\n",
    "df['TRUE_CLASS'] = trues\n",
    "df['PREDICT_CLASS'] = predicts\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prob values are still messy, we need to clean them\n",
    "\n",
    "import re\n",
    "probs3A = []\n",
    "probs3B = []\n",
    "\n",
    "for prob in probs:\n",
    "    x = prob.tolist()\n",
    "    probs3A.append(x[0][0])\n",
    "    probs3B.append(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into dataframe\n",
    "df['PROB_MASS'] = probs3A\n",
    "df['PROB_OTHER'] = probs3B\n",
    "# Sanity check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do some analysis. First let's determine how accurate the classifier is\n",
    "# by computing the rate of misclassifieds, and let's also identify those misclassifieds\n",
    "\n",
    "df['RESULT'] = ''\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if row['TRUE_CLASS'] != row['PREDICT_CLASS']:\n",
    "        df.set_value(index, 'RESULT', 'FALSE')\n",
    "    else:\n",
    "        df.set_value(index, 'RESULT', 'CORRECT')\n",
    "        \n",
    "misclassifieds = df[df['RESULT'] == 'FALSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple stuff, compute overall accuracy of classifier\n",
    "\n",
    "corrects = df[df['RESULT'] == 'CORRECT']\n",
    "accuracy = corrects.shape[0] / df.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texts most strongly predicted to belong to MASSES or OTHERS\n",
    "\n",
    "mass_df = df.sort_values(by=['PROB_MASS'], ascending=False)\n",
    "other_df = df.sort_values(by=['PROB_OTHER'], ascending=False)\n",
    "other_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Exercise! That's interesting MAS0206 is so strongly predicted to be OTHERS!\n",
    "# Let's take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Let's now look at specific feature weights based on the model\n",
    "# The idea is that we want a bit more granularity as to what specific features are driving our classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need some functions to help us compute these feature weights\n",
    "\n",
    "# Simple function to compute Z-score\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "    # edited from https://stats.stackexchange.com/questions/124096/two-samples-z-test-in-python\n",
    "    \n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "    \n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "    \n",
    "    return z, pval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another function to compute logistic regression weights on each feature (also does Z-test)\n",
    "\n",
    "canonic_c = 1.0 # value returning best f1\n",
    "\n",
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "    # Split dtms for ease in pipeline\n",
    "    \n",
    "    dtm_df_ = dtm_df_.loc[meta_df_.index.tolist()]\n",
    "    dtm_df_ = normalize_model(dtm_df_, dtm_df_)[0]\n",
    "    dtm_df_ = dtm_df_.dropna(axis=1, how='any')\n",
    "    \n",
    "    best_dtm = dtm_df_.loc[meta_df_[meta_df_['JOURNAL']=='Mas'].index.tolist()].as_matrix()\n",
    "    black_dtm = dtm_df_.loc[meta_df_[meta_df_['JOURNAL']=='Oth'].index.tolist()].as_matrix()\n",
    "    \n",
    "    pvals = [Ztest(best_dtm[:,i],black_dtm[:,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l1', C=canonic_c, class_weight = 'balanced')\n",
    "    clf.fit(dtm_df_, meta_df_['JOURNAL']=='Mas')\n",
    "    weights = clf.coef_[0]\n",
    "    \n",
    "    feature_df = pd.DataFrame()\n",
    "    \n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's probably a good idea to turn all the values in the DTM to standard units; that's what this function does\n",
    "\n",
    "def normalize_model(train_df_, test_df_):\n",
    "    \n",
    "    # Normalize each value by the sum of all values in its row\n",
    "    train_df_ = train_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    test_df_ = test_df_.apply(lambda x: x/sum(x), axis=1)\n",
    "    \n",
    "    # Get mean and stdev for each column\n",
    "    train_mean = np.mean(train_df_)\n",
    "    train_std = np.std(train_df_)\n",
    "\n",
    "    # Transform each value to standard units for its column\n",
    "    train_df_ = ( train_df_ - train_mean ) / train_std\n",
    "    test_df_ = ( test_df_ - train_mean ) / train_std\n",
    "    \n",
    "    train_df_ = train_df_.dropna(axis=1, how='any')\n",
    "    test_df_ = test_df_[train_df_.columns]\n",
    "    \n",
    "    return train_df_, test_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's recall our earlier dataframes\n",
    "\n",
    "# Metadata\n",
    "all_meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Term Matrix\n",
    "DTM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "all_meta.shape, DTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonferri adjustment\n",
    "sig_thresh = 0.01 #/ len(DTM.columns) Usually we use bonferri correction with so many features but for some reason\n",
    "# this wasn't necessary for this dataset, probably because this set is relatively small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature report\n",
    "import numpy as np\n",
    "feat_df = feat_pval_weight(all_meta, DTM)\n",
    "feat_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Report, TOP 25\n",
    "\n",
    "out = feat_df[ (feat_df['P_VALUE'] <= sig_thresh ) ].sort_values('LR_WEIGHT', ascending=False)\n",
    "# True is OTHER, False is MASS\n",
    "\n",
    "out2 = out['FEAT'].tolist()\n",
    "top_feats = out2[0:20]\n",
    "#print(\"TOP OTHER TEXT FEATURES: \")\n",
    "print(\"TOP MASSES TEXT FEATURES: \")\n",
    "for o in top_feats:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Close reading!\n",
    "# Let's go back to that MASSES text that was misclassified confidently to be an OTHERS text\n",
    "# Now we have a foundation or a better basis to close read it; we know the specific words \n",
    "# that are important in classifying it as OTHERS rather than MASSES, it's actual identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok so that's just some basic stuff we can do with classification/predictive modeling with a 2 class example\n",
    "# Obviously there is a ton more we can do; for example, in the lecture I gave on \"Race and Distant Reading,\"\n",
    "# I look at the feature variance between our two classes, and show how one group is far more variant than\n",
    "# the other. There's a lot more one can do with this method, of course, for literary studies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to do one more thing though. I increasingly find it useful to approach one's data from multiple perspectives,\n",
    "# to use multiple methods to see how different models understand the data.\n",
    "# There's another way we can determine the semantic differences between two groups of texts that is far simpler than\n",
    "# classification.\n",
    "# We can simply use a most distinctive words test.\n",
    "# So let's try that, just to see what kind of differnet results this produces versus our classification exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the Mann Whitney U test\n",
    "# A good overview/explanation and rationale for literary texts/poems is here:\n",
    "# https://tedunderwood.com/2011/11/09/identifying-the-terms-that-\n",
    "# characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/\n",
    "\n",
    "# Key point: \"In general, it gives less weight to raw frequency, and more weight to \n",
    "# the relative ubiquity of a term in different corpora.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first call up our data\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split our corpora back into MASSES and OTHERS again\n",
    "# This is the format my Mann Whitney U test function takes\n",
    "\n",
    "corpus1 = final_df[final_df['JOURNAL'] <= 'Mas']\n",
    "corpus2 = final_df[final_df['JOURNAL'] >= 'Oth']\n",
    "\n",
    "corpus1.shape, corpus2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's slice both corpora now to just get the word counts\n",
    "\n",
    "corpus1A = corpus1.iloc[:, 5:]\n",
    "corpus2A = corpus2.iloc[:, 5:]\n",
    "corpus1A.shape, corpus2A.shape\n",
    "corpus1A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the mann whitney utest\n",
    "# NB: For a function that takes some time, I like to see its progress, thus the i counter\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "i = 0\n",
    "out = []\n",
    "for column in corpus1A:\n",
    "    print(i)\n",
    "    vals = corpus1A[column].values\n",
    "    vals2 = corpus2A[column].values\n",
    "    mw = mannwhitneyu(vals, vals2)\n",
    "    mwStat = mw.statistic\n",
    "    mwRho = mwStat / corpus1A.shape[0] * corpus2A.shape[0]\n",
    "    out.append((column, mwStat, mwRho))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit into a dataframe\n",
    "words = []\n",
    "rho = []\n",
    "\n",
    "for item in out:\n",
    "    words.append(item[0])\n",
    "    rho.append(item[2])\n",
    "    \n",
    "mdw_df = pd.DataFrame(words, columns=['WORD'])\n",
    "mdw_df['MDW_RHO'] = rho\n",
    "len(rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = mdw_df.sort_values(\"MDW_RHO\", ascending=True)\n",
    "# False is MASSES, True is OTHERS\n",
    "df2[0:20] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What do you make of these results compared to the classification feature results?\n",
    "# What's your intuition as to why we got these results based on what you know of the model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
